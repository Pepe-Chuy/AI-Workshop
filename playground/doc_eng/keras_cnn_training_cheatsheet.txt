=== Tips & Considerations for Training CNNs with Keras ===

This guide is for beginners building and training CNNs (like for CIFAR-10).  
It provides simple but technical advice to help you avoid common mistakes.

------------------------------------------------------------
1. Data Preparation
------------------------------------------------------------
- Always normalize image data: divide pixel values by 255.0 to scale into [0,1].
- Shuffle training data to avoid learning order bias.
- Use one-hot encoding for labels (e.g., [0,0,1,0,...] for class 2).
- Data Augmentation: artificially increase dataset variety with rotations, flips, shifts, zooms.
  Example:
  keras.preprocessing.image.ImageDataGenerator(
      rotation_range=15,
      width_shift_range=0.1,
      height_shift_range=0.1,
      horizontal_flip=True
  )

------------------------------------------------------------
2. Model Design
------------------------------------------------------------
- Start simple: Conv → Pool → Dense → Softmax.
- Increase filters gradually (32 → 64 → 128).
- Use Dropout to avoid overfitting.
- BatchNormalization often improves stability and speed.
- Replace Flatten with GlobalAveragePooling2D to reduce parameters.

------------------------------------------------------------
3. Choosing Activations
------------------------------------------------------------
- Hidden layers: 'relu' is standard.
- Output layer: 'softmax' for multiclass classification (CIFAR-10).
- Try LeakyReLU/ELU if training is stuck.

------------------------------------------------------------
4. Choosing Optimizers
------------------------------------------------------------
- Adam: good default, adapts learning rates automatically.
- SGD + momentum: more control, sometimes higher accuracy with tuning.
- RMSprop: works well for CNNs too.
- Always monitor learning rate; too high = unstable, too low = slow.

------------------------------------------------------------
5. Training Process
------------------------------------------------------------
- Epochs: 20–50 for CIFAR-10, but use EarlyStopping to avoid overfitting.
- Batch size: common values = 32, 64, 128. Larger batches train faster but need more memory.
- Learning rate: start with 0.001 (Adam default). Use LearningRateScheduler or ReduceLROnPlateau.
- Always split data into training and validation sets.

------------------------------------------------------------
6. Monitoring Training
------------------------------------------------------------
- Watch training vs validation accuracy/loss curves.
- If validation loss goes up while training loss goes down → overfitting.
- Use callbacks:
  - EarlyStopping: stop when validation stops improving.
  - ModelCheckpoint: save best model.
  - ReduceLROnPlateau: lower learning rate if stuck.

------------------------------------------------------------
7. Common Mistakes to Avoid
------------------------------------------------------------
- Forgetting to normalize data → model won’t converge.
- Using too deep a model too early → overfitting, slow training.
- Not shuffling data → poor generalization.
- Training too few epochs → underfitting.

------------------------------------------------------------
8. Evaluation & Testing
------------------------------------------------------------
- Use model.evaluate() on test set for unbiased results.
- Always compare accuracy on training, validation, and test.
- Visualize misclassified images to understand model weaknesses.

------------------------------------------------------------
9. Saving & Loading Models
------------------------------------------------------------
- Save model after training:
  model.save("cnn_model.h5")
- Load later:
  keras.models.load_model("cnn_model.h5")

------------------------------------------------------------
10. Practical Tips for First-Timers
------------------------------------------------------------
- Start small and verify model runs before making it complex.
- Change one thing at a time (filters, dropout, optimizer).
- Always keep track of accuracy/loss across epochs.
- Don’t chase 100% accuracy; focus on learning process.
- Document your experiments (what worked, what didn’t).

------------------------------------------------------------
=== Quick Checklist Before Training ===
[ ] Normalize data (x/255.0)
[ ] One-hot encode labels
[ ] Split into train/val/test
[ ] Add dropout & batch norm where needed
[ ] Use 'relu' + 'softmax'
[ ] Start with Adam optimizer, lr=0.001
[ ] Monitor validation accuracy & loss
[ ] Save best model with checkpoints

------------------------------------------------------------
=== Example: Small CNN with Two Convolutional Blocks ===
```python
from tensorflow.keras import layers, models

def create_small_cnn(input_shape=(32,32,3), num_classes=10):
    model = models.Sequential()

    # First Conv Block
    model.add(layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=input_shape))
    model.add(layers.BatchNormalization())
    model.add(layers.Conv2D(32, (3,3), activation='relu', padding='same'))
    model.add(layers.MaxPooling2D((2,2)))
    model.add(layers.Dropout(0.25))

    # Second Conv Block
    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))
    model.add(layers.MaxPooling2D((2,2)))
    model.add(layers.Dropout(0.25))

    # Output Layers
    model.add(layers.GlobalAveragePooling2D())
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.Dropout(0.5))
    model.add(layers.Dense(num_classes, activation='softmax'))

    return model

# Example usage
cnn_model = create_small_cnn()
cnn_model.summary()
